# PlaterWorker - OpenClaw con soporte OpenRouter

PlaterWorker es un fork de [moltworker](https://github.com/cloudflare/moltworker) que a√±ade soporte nativo para [OpenRouter](https://openrouter.ai) como proveedor de modelos AI. Permite ejecutar [OpenClaw](https://github.com/openclaw/openclaw) (anteriormente Moltbot/Clawdbot) en [Cloudflare Sandbox](https://developers.cloudflare.com/sandbox/) con acceso a m√∫ltiples modelos de IA a trav√©s de OpenRouter.

![PlaterWorker](./assets/logo.png)

> **‚ö†Ô∏è ADVERTENCIA IMPORTANTE**: Las modificaciones para soportar OpenRouter fueron realizadas principalmente mediante IA y pueden contener errores. Este proyecto es experimental y debe usarse con precauci√≥n. Se recomienda revisar el c√≥digo antes de usar en producci√≥n.

> **Experimental:** Este es un concepto de prueba que demuestra que OpenClaw puede correr en Cloudflare Sandbox. No est√° oficialmente soportado y puede dejar de funcionar sin previo aviso. √öselo bajo su propio riesgo.

## ‚≠ê Cambios principales respecto a moltworker

### Soporte OpenRouter
- **Configuraci√≥n simplificada**: OpenRouter se configura autom√°ticamente como proveedor por defecto
- **M√∫ltiples modelos disponibles**: Acceso a modelos de Anthropic, OpenAI, DeepSeek, Qwen, Google, X.AI y m√°s
- **Auto-routing inteligente**: Usa `openrouter/auto` o `openrouter/free` para routing autom√°tico
- **Aliases cortos**: Comandos simples como `/qwen`, `/deep`, `/haiku`, `/grok`

### Modelos preconfigurados
- **Auto-routing**: `auto`, `free`
- **C√≥digo**: `qwen`, `qwenfree`, `devstral`, `mimofree`, `grokcode`
- **General**: `deep`, `kimi`, `flash`
- **Claude**: `haiku`, `sonnet`
- **OpenAI**: `mini`, `gpt`
- **Razonamiento**: `think`, `qwq`
- **GLM**: `glm-4.7`, `glmfree`

### Workspace modificado
- Workspace movido a `/root/.clawdbot/workspace` para mejor organizaci√≥n
- Backup completo a R2 incluye workspace y configuraciones

## üìã Requisitos

- [Workers Paid plan](https://www.cloudflare.com/plans/developer-platform/) ($5 USD/mes) ‚Äî requerido para Cloudflare Sandbox
- [Cuenta OpenRouter](https://openrouter.ai/) ‚Äî para acceso a modelos IA
- O [Anthropic API key](https://console.anthropic.com/) ‚Äî como alternativa

Las siguientes funcionalidades de Cloudflare tienen tiers gratuitos:
- Cloudflare Access (autenticaci√≥n)
- Browser Rendering (navegaci√≥n en navegador)
- AI Gateway (opcional, para routing/analytics)
- R2 Storage (opcional, para persistencia)

## üöÄ Instalaci√≥n r√°pida

### Opci√≥n 1: Usar OpenRouter (Recomendado)

```bash
# Instalar dependencias
npm install

# Configurar OpenRouter API Key
npx wrangler secret put OPENROUTER_API_KEY

# Configurar base URL para OpenRouter
npx wrangler secret put AI_GATEWAY_BASE_URL
# Ingresar: https://openrouter.ai/api/v1

# Generar token de gateway
export MOLTBOT_GATEWAY_TOKEN=$(openssl rand -hex 32)
echo "Token del gateway: $MOLTBOT_GATEWAY_TOKEN"
echo "$MOLTBOT_GATEWAY_TOKEN" | npx wrangler secret put MOLTBOT_GATEWAY_TOKEN

# Desplegar
npm run deploy
```

### Opci√≥n 2: Usar Anthropic API directa

```bash
# Instalar dependencias
npm install

# Configurar Anthropic API Key
npx wrangler secret put ANTHROPIC_API_KEY

# Generar token de gateway
export MOLTBOT_GATEWAY_TOKEN=$(openssl rand -hex 32)
echo "Token del gateway: $MOLTBOT_GATEWAY_TOKEN"
echo "$MOLTBOT_GATEWAY_TOKEN" | npx wrangler secret put MOLTBOT_GATEWAY_TOKEN

# Desplegar
npm run deploy
```

### Opci√≥n 3: Usar Nvidia API (Kimi 2.5 y NIM models)

```bash
# Instalar dependencias
npm install

# Configurar Nvidia API Key (obtener en https://build.nvidia.com)
npx wrangler secret put NVIDIA_API_KEY

# Opcional: Configurar base URL personalizada
npx wrangler secret put AI_GATEWAY_BASE_URL
# Ingresar: https://integrate.api.nvidia.com/v1

# Generar token de gateway
export MOLTBOT_GATEWAY_TOKEN=$(openssl rand -hex 32)
echo "Token del gateway: $MOLTBOT_GATEWAY_TOKEN"
echo "$MOLTBOT_GATEWAY_TOKEN" | npx wrangler secret put MOLTBOT_GATEWAY_TOKEN

# Desplegar
npm run deploy
```

### Opci√≥n 4: Usar AI Gateway de Cloudflare

```bash
# Instalar dependencias
npm install

# Configurar AI Gateway
npx wrangler secret put AI_GATEWAY_API_KEY
npx wrangler secret put AI_GATEWAY_BASE_URL
# Ingresar: https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/anthropic
# O para OpenRouter: https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/openrouter

# Generar token de gateway
export MOLTBOT_GATEWAY_TOKEN=$(openssl rand -hex 32)
echo "Token del gateway: $MOLTBOT_GATEWAY_TOKEN"
echo "$MOLTBOT_GATEWAY_TOKEN" | npx wrangler secret put MOLTBOT_GATEWAY_TOKEN

# Desplegar
npm run deploy
```

Despu√©s de desplegar, abre el Control UI con tu token:

```
https://your-worker.workers.dev/?token=YOUR_GATEWAY_TOKEN
```

Reemplaza `your-worker` con tu subdominio de worker y `YOUR_GATEWAY_TOKEN` con el token que generaste.

**Nota:** La primera petici√≥n puede tomar 1-2 minutos mientras el contenedor se inicia.

## üîê Configuraci√≥n de seguridad

> **Importante:** No podr√°s usar el Control UI hasta completar estos pasos:
> 1. [Configurar Cloudflare Access](#configurar-cloudflare-access) para proteger el admin UI
> 2. [Emparejar tu dispositivo](#emparejamiento-de-dispositivos) v√≠a admin UI en `/_admin/`

Tambi√©n se recomienda [habilitar almacenamiento R2](#almacenamiento-persistente-r2) para que los dispositivos emparejados y el historial persistan entre reinicios del contenedor.

### Configurar Cloudflare Access

Ver secci√≥n completa en la [documentaci√≥n original de moltworker](https://github.com/cloudflare/moltworker#setting-up-the-admin-ui).

Resumen:
1. Habilitar Cloudflare Access en tu worker
2. Configurar dominio y audiencia (AUD)
3. Establecer secretos:
```bash
npx wrangler secret put CF_ACCESS_TEAM_DOMAIN
npx wrangler secret put CF_ACCESS_AUD
```
4. Redesplegar

### Emparejamiento de dispositivos

Por defecto, cada nuevo dispositivo debe ser aprobado en `/_admin/` antes de poder usar el asistente. Esto aplica para:
- Navegadores web
- CLI clients
- Bots de Telegram/Discord/Slack (DMs)

## üíæ Almacenamiento persistente (R2)

Por defecto, los datos de moltbot (configs, dispositivos emparejados, historial de conversaciones) se pierden cuando el contenedor se reinicia. Para habilitar persistencia:

### 1. Crear token de API R2

1. Ir a **R2** > **Overview** en el [Dashboard de Cloudflare](https://dash.cloudflare.com/)
2. Click en **Manage R2 API Tokens**
3. Crear nuevo token con permisos **Object Read & Write**
4. Seleccionar el bucket `moltbot-data` (se crea autom√°ticamente)
5. Copiar **Access Key ID** y **Secret Access Key**

### 2. Configurar secretos

```bash
npx wrangler secret put R2_ACCESS_KEY_ID
npx wrangler secret put R2_SECRET_ACCESS_KEY
npx wrangler secret put CF_ACCOUNT_ID
```

### C√≥mo funciona

El almacenamiento R2 usa un enfoque de backup/restore:

**Al iniciar el contenedor:**
- Si R2 est√° montado y contiene datos de backup, se restaura a `/root/.clawdbot`
- Incluye workspace completo en `/root/.clawdbot/workspace`

**Durante operaci√≥n:**
- Un cron job corre cada 5 minutos para sincronizar a R2
- Tambi√©n puedes hacer backup manual desde el admin UI

**En el admin UI:**
- Ver√°s "Last backup: [timestamp]"
- Click en "Backup Now" para sincronizaci√≥n inmediata

## üéÆ Usar modelos de OpenRouter

### Cambiar modelo en el chat

Usa comandos con `/` seguido del alias del modelo:

```
/qwen ¬øC√≥mo optimizo este c√≥digo Python?
/deep Explica arquitectura de microservicios
/haiku Responde brevemente
/think Resuelve este problema complejo (razonamiento)
```

### Modelos disponibles

| Alias | Modelo | Descripci√≥n |
|-------|--------|-------------|
| `auto` | openrouter/auto | Auto-routing inteligente |
| `free` | openrouter/free | Free-routing |
| `qwen` | qwen-2.5-coder-32b | Especialista en c√≥digo |
| `qwenfree` | qwen-2.5-coder-32b:free | Versi√≥n gratuita |
| `deep` | deepseek-chat-v3 | Prop√≥sito general |
| `devstral` | mistralai/devstral-small:free | C√≥digo (free) |
| `grok` | x-ai/grok-4.1-fast | Agentic/Tools |
| `haiku` | claude-3.5-haiku | Claude r√°pido |
| `sonnet` | claude-sonnet-4 | Claude avanzado |
| `mini` | gpt-4o-mini | OpenAI econ√≥mico |
| `gpt` | gpt-4o | OpenAI est√°ndar |
| `think` | deepseek-reasoner | Razonamiento profundo |
| `flash` | gemini-2.0-flash | Google r√°pido |

### Modelos Nvidia NIM disponibles

Con Nvidia API, tienes acceso a modelos adicionales optimizados:

| Alias | Modelo | Descripci√≥n |
|-------|--------|-------------|
| `kimi` | moonshotai/kimi-k2.5 | Kimi 2.5 - Excelente contexto largo |
| `moonshot8k` | moonshot-v1-8k | Moonshot 8K context |
| `moonshot32k` | moonshot-v1-32k | Moonshot 32K context |
| `moonshot128k` | moonshot-v1-128k | Moonshot 128K context |
| `llama70b` | llama-3.3-70b | Meta Llama 70B |
| `llama405b` | llama-3.1-405b | Meta Llama 405B |
| `mistral` | mistral-large-2 | Mistral Large 2 |
| `mixtral` | mixtral-8x7b | Mixtral MoE |
| `nemotron` | llama-nemotron-70b | Nvidia Nemotron |
| `deepseek-r1` | deepseek-r1 | DeepSeek R1 reasoning |

**Uso**:
```
/kimi Explica este concepto en detalle
/llama405b Analiza este c√≥digo complejo
/deepseek-r1 Resuelve este problema (con razonamiento)
```

### Configurar modelo por defecto

El modelo por defecto es `openrouter/free` (free-routing autom√°tico). Para cambiarlo:

1. Editar `moltbot.json.template` antes de desplegar
2. O modificar `/root/.clawdbot/clawdbot.json` en el contenedor despu√©s del primer inicio
3. O usar variable de entorno `MOLTBOT_DEFAULT_MODEL`

## üöÄ Nvidia NIM - Configuraci√≥n avanzada

Nvidia NIM (Nvidia Inference Microservices) proporciona acceso a modelos optimizados de m√∫ltiples proveedores, incluyendo Kimi 2.5 de Moonshot AI.

### Obtener API Key

1. Visita [build.nvidia.com](https://build.nvidia.com)
2. Crea una cuenta o inicia sesi√≥n
3. Navega a la secci√≥n de API Keys
4. Genera una nueva API key
5. Copia la key (formato: `nvapi-XXXXXXXXXXXX`)

### Configuraci√≥n b√°sica

```bash
# Configurar API key
npx wrangler secret put NVIDIA_API_KEY
# Pegar tu key: nvapi-XXXXXXXXXXXX

# Redesplegar
npm run deploy
```

### Usar con AI Gateway de Cloudflare

Para routing, caching y analytics:

```bash
# Configurar ambas variables
npx wrangler secret put NVIDIA_API_KEY
npx wrangler secret put AI_GATEWAY_BASE_URL
# Ingresar: https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/nvidia
```

### Caracter√≠sticas especiales de Kimi 2.5

El modelo Kimi 2.5 soporta el par√°metro `thinking` para razonamiento expl√≠cito:

```javascript
// En el c√≥digo, esto se configura autom√°ticamente
{
  "chat_template_kwargs": {"thinking": true}
}
```

Para usar thinking mode en el chat:
```
/kimi --think Analiza este problema paso a paso
```

### Modelos recomendados seg√∫n uso

- **Contexto largo**: `/kimi`, `/moonshot128k` - Hasta 128K tokens
- **C√≥digo**: `/nemotron`, `/llama70b` - Optimizados para programaci√≥n
- **Razonamiento**: `/deepseek-r1` - Chain-of-thought expl√≠cito
- **General**: `/mistral`, `/llama405b` - Mejor calidad general
- **R√°pido**: `/moonshot8k`, `/mixtral` - Respuestas m√°s r√°pidas

## üìö Documentaci√≥n adicional

La mayor√≠a de la funcionalidad es id√©ntica a moltworker original. Consulta la [documentaci√≥n completa de moltworker](https://github.com/cloudflare/moltworker) para:

- Admin UI y gesti√≥n de dispositivos
- Canales de chat (Telegram, Discord, Slack)
- Browser automation (CDP)
- Skills personalizados
- Debug endpoints
- Cloudflare AI Gateway
- Troubleshooting

## üîß Diferencias t√©cnicas vs moltworker

### Archivos modificados
- `start-moltbot.sh`: L√≥gica de configuraci√≥n de OpenRouter
- `moltbot.json.template`: Workspace y modelo por defecto
- `src/types.ts`: Tipo `OPENROUTER_API_KEY`
- `src/gateway/env.ts`: Propagaci√≥n de `OPENROUTER_API_KEY`
- `Dockerfile`: Build cache bust actualizado

### Cambios en configuraci√≥n
- Workspace: `/root/clawd` ‚Üí `/root/.clawdbot/workspace`
- Modelo por defecto: `anthropic/claude-opus-4-5` ‚Üí `openrouter/free`
- Provider por defecto: Anthropic ‚Üí OpenRouter

## ‚ö†Ô∏è Problemas conocidos

### Debido a modificaciones por IA
- La configuraci√≥n de modelos en `start-moltbot.sh` puede tener redundancias
- Algunos nombres de modelos pueden quedar desactualizados
- La l√≥gica de detecci√≥n de provider puede fallar en casos edge

### Recomendaciones
1. **Probar en desarrollo primero** con `wrangler dev`
2. **Revisar logs** con `npx wrangler tail` despu√©s de desplegar
3. **Verificar configuraci√≥n** en el admin UI despu√©s del primer inicio
4. **Backup manual** desde admin UI antes de hacer cambios grandes

### Si algo falla
1. Revisar logs: `npx wrangler tail`
2. Verificar secretos: `npx wrangler secret list`
3. Limpiar cache de build: Editar comentario `# Build cache bust` en Dockerfile
4. Redesplegar: `npm run deploy`

## üêõ Reportar problemas

Este es un proyecto experimental y no oficial. Los problemas deben reportarse en el fork, no en el repositorio original de moltworker.

## üìÑ Licencia

Este proyecto mantiene la misma licencia que moltworker (MIT). Ver [LICENSE](LICENSE) para detalles.

## üôè Cr√©ditos

- Proyecto base: [moltworker](https://github.com/cloudflare/moltworker) por Cloudflare
- OpenClaw: [openclaw/openclaw](https://github.com/openclaw/openclaw)
- Modificaciones OpenRouter: Realizadas principalmente con asistencia de IA

---

**‚ö†Ô∏è Recordatorio**: Las modificaciones fueron hechas con IA y pueden contener errores. Usa bajo tu propio riesgo y revisa el c√≥digo antes de producci√≥n.
